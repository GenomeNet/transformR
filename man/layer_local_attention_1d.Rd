% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/attention-layers.R
\name{layer_local_attention_1d}
\alias{layer_local_attention_1d}
\title{Strided block local self-attention.}
\usage{
layer_local_attention_1d(
  q,
  k,
  v,
  block_length = 1024L,
  filter_width = 100L,
  name = "local_attention_1d"
)
}
\description{
The sequence is divided into blocks of length block_length.
Attention for agiven query position can see all memory positions
in the corresponding block and filter_width many positions to
the left and right of the block.
q Tensor [batch, heads, length, depth_k]
k Tensor [batch, heads, length, depth_k]
v Tensor [batch, heads, length, depth_v]
Returns Tensor [batch, heads, length, depth_v]
}
